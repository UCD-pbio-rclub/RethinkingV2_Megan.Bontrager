---
title: "Chapter7"
author: "Megan Bontrager"
date: "7/10/2019"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```





## Homework problems (added 24 July)

#### 1. Three islands, five birb species. First, compute entropy of each island's birb distribution. 


```{r}
i1 = c(0.2, 0.2, 0.2, 0.2, 0.2)
-sum(i1*log(i1))

i2 = c(0.8, 0.1, 0.05, 0.025, 0.025)
-sum(i2*log(i2))

i3 = c(0.05, 0.15, 0.7, 0.05, 0.05)
-sum(i3*log(i3))

```



|         | Birb A | Birb B | Birb C | Birb D | Birb E | Entropy |
|---------|--------|--------|--------|--------|--------|---------|         
|Island 1 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 1.61 |
|Island 2 | 0.8 | 0.1 | 0.05 | 0.025 | 0.025 | 0.743 |
|Island 3 |0.05 | 0.15 | 0.7 | 0.05 | 0.05 | 0.984 |

Island 1 has the highest entropy - because the proportions are pretty even there is high uncertainty about which birb would pop up in a random draw. Island 2 has the lowest entropy. Since most of the bribs are species A, there is lower uncertainty about which species any given bird will be. 

Second, use each island's birb distribution to predict the other two islands. 
- compute K-L divergence of each island from each of the others.

```{r}
i1 = c(0.2, 0.2, 0.2, 0.2, 0.2)

i2 = c(0.8, 0.1, 0.05, 0.025, 0.025)

i3 = c(0.05, 0.15, 0.7, 0.05, 0.05)

Dkl_12 = sum(i1*(log(i1) - log(i2)))
Dkl_13 = sum(i1*(log(i1) - log(i3)))

Dkl_21 = sum(i2*(log(i2) - log(i1)))
Dkl_23 = sum(i2*(log(i2) - log(i3)))

Dkl_31 = sum(i3*(log(i3) - log(i1)))
Dkl_32 = sum(i3*(log(i3) - log(i2)))

```

|              | q = Island 1 | q = Island 2 | q = Island 3 | 
|--------------|--------|--------|--------|
| p = Island 1 |     | 0.970    | 0.639  | 
| p = Island 2 |  0.866    |     | 2.01 | 
| p = Island 3 |  0.626  | 1.84    |  |

Which island predicts the others best? Why?

Island 1 predicts the others best because it "expects" each species of birds evenly, so it's not particularly "surprised" by the other islands' values.

#### 2. Compare models m6.9 and m6.10 with WAIC or LOO. Which model is expected to make better predictions? Which model provides the correct causal inference about the influence of age on happiness?

``` {r}
library(rethinking)
d = sim_happiness(seed = 1977, N_years = 1000)
precis(d)

d2 = d[d$age >17, ]
d2$A = (d2$age - 18)/(65-18)



```



## Exercises

### 7E1. State the three motivating criteria that define information entropy. Try to express each in your own words.

- The measure should be continuous so it can change very incrementally as our information changes. 
- The measure needs to increase as the possible events increases. If there are three possible outcomes, that scenario should have higher uncertainty than if there were just two. [Question: what about situations where it's not A or B that we are trying to predict, but a continuous response?]
- The measure should be additive. If there are multiple responses we're trying to predict, then the summed uncertainty of each response should be equal to the total uncertainty. 


### 7E2. Suppose a coin is weighted such that, when it is tossed and lands on a table, it comes up heads 70% of the time. What is the entropy of this coin?

```{r}
p = c(0.3, 0.7)
-sum(p*log(p))
```
0.61

### 7E3. Suppose a four-sided die is loaded such that, when tossed onto a table, it shows “1” 20%, “2” 25%, ”3” 25%, and ”4” 30% of the time. What is the entropy of this die?

```{r}
p = c(0.2, 0.25, 0.25, 0.3)
-sum(p*log(p))
```
1.38

### 7E4. Suppose another four-sided die is loaded such that it never shows “4”.  e other three sides show equally often. What is the entropy of this die?

Assume that if pi = 0, pi*log(pi) -> 0, so can ignore.

```{r}
p = c(1/3, 1/3, 1/3)
-sum(p*log(p))
```
1.10


### 7M1. Write down and compare the definitions of AIC, DIC, and WAIC. Which of these criteria is most general? Which assumptions are required to transform a more general criterion into a less general one?

From least to most general:

AIC: an estimate of the average out of sample deviance. Assumes flat priors, gaussian posterior, sample size >> number of parameters.

D_train + 2p = 2lppd + 2p

p = number of free parameters (<- what is free?)

DIC: an estimate of the average out of sample deviance. Assumes gaussian posterior, sample size >> number of parameters, but can accomodate informative piors.

WAIC: an approximation of the out of sample deviance that converges to the LOO CV approximation of a large sample. No assumptions about the shape of the posterior and can accomodate flat priors. 


### 7M2. Explain the difference between model selection and model averaging. What information is lost under model selection? What information is lost under model averaging?

Model selection means trying to pick the "best" model, by choosing the model with the lowest IC value. By selecting just one model and not assessing the others, you lose information about the relative performance of the different models you have fit. If many models perform similarly, that's relevant information because maybe all are valid descriptions of the data. Similarly, if one model is far better than the others, that gives you more confidence that it's the best among those you compared. 

A better approach is to compare models and retain information about their relative performance. You could then weight the models by their performance and average the parameter esitmates across them. 


### 7M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure.



### 7M4. What happens to the effective number of parameters, as measured by DIC or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure.

The "effective number of parameters" is equivalent to the penalty term in these calculations. If the prior is more concentrated

### 7M5.Provide an informal explanation of why informative priors reduce overfitting.

Informative priors reduce overfitting because they prevent the model from learning too much from the data, and dampen the range of parameter values that the model will give. 

### 7M6. Provide an information explanation of why overly informative priors result in underfitting.

If parameters are too informative, they will prevent the sample from learning from the data. 




## Notes:

Review:

Index variables for categorical variables:
- Dummy: 0, 1 for each possible state results in basically n category intercepts
- Assumes more uncertainty for some variables
- Instead: index variables
- Same prior for all categories
Example:
h ~ dnorm (mu, sigma),
mu <- a[sex],
a[sex] ~ dnorm(prior),
sigma ~ dunif(prior)
result is estimates of alpha for each sex.
can calculate distribution of differences between categories with subtraction of columns in the posterior

Fork: common cause of 2 things (open)
- If both things are in model, only 1 might have non-zero parameter estimate
- Either thing alone might have an effect. 
- Once we know A, little additional information provided by B, but if we know B, A is still useful. 

Pipe: middle term mediates relationship between 2 other variables (open)

Collider: Z is a common result of 2 other variables (closed)

Descendant: A descendant of the Z exists and is like a weaker version of Z 



